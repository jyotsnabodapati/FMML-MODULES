{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotsnabodapati/FMML-MODULES/blob/main/FMML_Module4_Lab3_June2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad\n",
        "# Module 4: Perceptron and Gradient Descent\n",
        "## Lab 3: Gradient Descent\n",
        "\n",
        "Gradient descent is a very important algorithm to understand, as it underpins many of the more advanced algorithms used in Machine Learning and Deep Learning.\n",
        "\n",
        "A brief overview of the algorithm is\n",
        "\n",
        "\n",
        "*   start with a random initialization of the solution.\n",
        "*   incrementally change the solution by moving in the direction of negative gradient of the objective function.\n",
        "*   repeat the previous step until some convergence criteria is met.\n",
        "\n",
        "The key equation for change in weight is:\n",
        "$$w^{k+1} \\leftarrow w^k - \\eta \\Delta J$$\n",
        "\n",
        "In this lab, we will discuss stochastic gradient descent, mini-batch gradient descent and batch gradient descent.\n"
      ],
      "metadata": {
        "id": "XYxxkQg6xCjD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr-MnaGs7JmZ"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ob_zZms7VOu"
      },
      "source": [
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Kix4bcChiy"
      },
      "source": [
        "# Creating the Data\n",
        "\n",
        "Let's generate some data with:\n",
        "\\begin{equation} y_0= 4 \\end{equation}\n",
        "\\begin{equation} y_1= 3 \\end{equation}\n",
        "\n",
        "and also add some noise to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtAS7eFZ9hX6"
      },
      "source": [
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD95NaF-CxM-"
      },
      "source": [
        "Let's also plot the data we just created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "3IiEP4BQ7Wja",
        "outputId": "b71bf78b-5473-4641-e57c-b37fcf0f9d2e"
      },
      "source": [
        "plt.plot(X, y, 'b.')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y', rotation=0)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-531b667ff8b4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScwxpouoDDyZ"
      },
      "source": [
        "## Cost Function\n",
        "\n",
        "The equation for calculating cost function is as shown below. The cost function is only for linear regression. For other algorithms, the cost function will be different and the gradients would have to be derived from the cost functions\n",
        "\n",
        "\\begin{equation}\n",
        "J(y_{pred}) = \\frac{1}{2} m \\sum_{i=1}^{m} (h(y_{pred})^{(i)} - y^{(i)})^2\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUeTUAXH7ZaV"
      },
      "source": [
        "def cal_cost(y_pred, X, y):\n",
        "    '''\n",
        "    Calculates the cost for given X and Y.\n",
        "    y_pred = Vector of y_preds\n",
        "    X = Row of X's np.zeros((2, j))\n",
        "    y = Actual y's np.zeros((2, 1))\n",
        "\n",
        "    where:\n",
        "        j is the no of features\n",
        "    '''\n",
        "\n",
        "    m = len(y)\n",
        "\n",
        "    predictions = X.dot(y_pred)\n",
        "    cost = (1 / 2 * m) * np.sum(np.square(predictions - y))\n",
        "\n",
        "    return cost"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcXqsVNpDbKC"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "\\begin{equation}\n",
        "y_{pred_0}: = y_{pred_0} -\\alpha . (1/m .\\sum_{i=1}^{m}(h(y_{pred}^{(i)} - y^{(i)}).X_0^{(i)})\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "y_{pred_1}: = y_{pred_1} -\\alpha . (1/m .\\sum_{i=1}^{m}(h(y_{pred}^{(i)} - y^{(i)}).X_0^{(i)})\n",
        "\\end{equation}\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\\begin{equation}\n",
        "y_{pred_j}: = y_{pred_j} -\\alpha . (1/m .\\sum_{i=1}^{m}(h(y_{pred}^{(i)} - y^{(i)}).X_0^{(i)})\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwxBFXP88NBW"
      },
      "source": [
        "def gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=100):\n",
        "    '''\n",
        "    X = Matrix of X with added bias units\n",
        "    y = Vector of Y\n",
        "    y_pred = Vector of y_preds np.random.randn(j, 1)\n",
        "    learning_rate\n",
        "    iterations = no of iterations\n",
        "\n",
        "    Returns the final y_pred vector and array of cost history over no of iterations\n",
        "    '''\n",
        "\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    y_pred_history = np.zeros((iterations, 2))\n",
        "\n",
        "    for it in range(iterations):\n",
        "        prediction = np.dot(X, y_pred)\n",
        "        y_pred = y_pred - (1 / m) * learning_rate * (X.T.dot((prediction - y)))\n",
        "        y_pred_history[it,:] = y_pred.T\n",
        "        cost_history[it]  = cal_cost(y_pred, X, y)\n",
        "\n",
        "    return y_pred, cost_history, y_pred_history"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iSohSB2EtK1"
      },
      "source": [
        "Let's do 1000 iterations with a learning rate of 0.01.\n",
        "We will start with a random prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "18AX7hrU8bv5",
        "outputId": "c5305e7c-a485-488e-d7ca-175eb07fe161"
      },
      "source": [
        "lr = 0.01\n",
        "n_iter = 1000\n",
        "\n",
        "y_pred = np.random.randn(2,1)\n",
        "X_b = np.c_[np.ones((len(X), 1)), X]\n",
        "y_pred, cost_history, y_pred_history = gradient_descent(X_b, y, y_pred, lr, n_iter)\n",
        "\n",
        "print('y_pred[0]: {:0.3f}\\ny_pred[1]: {:0.3f}'.format(y_pred[0][0], y_pred[1][0]))\n",
        "print('Final error: {:0.3f}'.format(cost_history[-1]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ae9e71f9b10c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7fao2MaE216"
      },
      "source": [
        "Plotting the error vs Number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrkrAAbk8hIs"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "ax.set_ylabel('Error')\n",
        "ax.set_xlabel('Number of iterations')\n",
        "\n",
        "ax.plot(range(n_iter), cost_history, 'b.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG5tWAy-FCaW"
      },
      "source": [
        "Zooming in..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ7BoFHy8kTk"
      },
      "source": [
        "fig,ax = plt.subplots(figsize=(10,8))\n",
        "ax.plot(range(200), cost_history[:200], 'b.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYhOp3fjnh2G"
      },
      "source": [
        "# Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Batch Gradient Descent we were considering all the examples for every step of Gradient Descent. But what if our dataset is very huge. Deep learning models crave for data. The more the data the more chances of a model to be good. Suppose our dataset has 5 million examples, then just to take one step the model will have to calculate the gradients of all the 5 million examples. This does not seem an efficient way. To tackle this problem we have Stochastic Gradient Descent. In Stochastic Gradient Descent (SGD), we consider just one example at a time to take a single step."
      ],
      "metadata": {
        "id": "10N2dcwWUctJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVwD7Cqw8m1d"
      },
      "source": [
        "def stocashtic_gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=10):\n",
        "    '''\n",
        "    X = Matrix of X with added bias units\n",
        "    y = Vector of Y\n",
        "    y_pred = Vector of y_pred np.random.randn(j,1)\n",
        "    learning_rate\n",
        "    iterations = no of iterations\n",
        "\n",
        "    Returns the final y_pred vector and array of cost history over no of iterations\n",
        "    '''\n",
        "\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "\n",
        "    for it in range(iterations):\n",
        "        cost = 0.0\n",
        "\n",
        "        for i in range(m):\n",
        "            rand_ind = np.random.randint(0,m)\n",
        "            X_i = X[rand_ind, :].reshape(1, X.shape[1])\n",
        "            y_i = y[rand_ind].reshape(1,1)\n",
        "            prediction = np.dot(X_i, y_pred)\n",
        "\n",
        "            y_pred = y_pred - (1 / m) * learning_rate *(X_i.T.dot((prediction - y_i)))\n",
        "            cost += cal_cost(y_pred, X_i, y_i)\n",
        "\n",
        "        cost_history[it]  = cost\n",
        "\n",
        "    return y_pred, cost_history"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Yk6pfB5c8tPz",
        "outputId": "8a92d01d-d28f-4006-a6a4-b2c281220c4a"
      },
      "source": [
        "lr = 0.5\n",
        "n_iter = 50\n",
        "y_pred = np.random.randn(2, 1)\n",
        "X_b = np.c_[np.ones((len(X), 1)), X]\n",
        "y_pred, cost_history = stocashtic_gradient_descent(X_b, y, y_pred, lr, n_iter)\n",
        "\n",
        "print('y_pred[0]: {:0.3f}\\ny_pred[1]: {:0.3f}'.format(y_pred[0][0], y_pred[1][0]))\n",
        "print('Final error: {:0.3f}'.format(cost_history[-1]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3dc7ac814300>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstocashtic_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "YiJUgS7o8u2e",
        "outputId": "360c37eb-f97f-420f-d855-faf931faae77"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "\n",
        "ax.set_ylabel('Error')\n",
        "ax.set_xlabel('Number of iterations')\n",
        "y_pred = np.random.randn(2,1)\n",
        "\n",
        "ax.plot(range(n_iter), cost_history, 'b.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7b5300276da0>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScckWktynk1o"
      },
      "source": [
        "# Mini Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen the Batch Gradient Descent. We have also seen the Stochastic Gradient Descent. Batch Gradient Descent can be used for smoother curves. SGD can be used when the dataset is large. Batch Gradient Descent converges directly to minima. SGD converges faster for larger datasets. But, since in SGD we use only one example at a time, we cannot implement the vectorized implementation on it. This can slow down the computations. To tackle this problem, a mixture of Batch Gradient Descent and SGD is used.\n",
        "Neither we use all the dataset all at once nor we use the single example at a time. We use a batch of a fixed number of training examples which is less than the actual dataset and call it a mini-batch. Doing this helps us achieve the advantages of both the former variants we saw."
      ],
      "metadata": {
        "id": "ZTVz-QssUkuE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JtxFVL78wEm"
      },
      "source": [
        "def minibatch_gradient_descent(X, y, y_pred, learning_rate=0.01, iterations=10, batch_size=20):\n",
        "    '''\n",
        "    X = Matrix of X without added bias units\n",
        "    y = Vector of Y\n",
        "    y_pred = Vector of y_preds np.random.randn(j, 1)\n",
        "    learning_rate\n",
        "    iterations = no of iterations\n",
        "\n",
        "    Returns the final theta vector and array of cost history over no of iterations\n",
        "    '''\n",
        "\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    n_batches = int(m / batch_size)\n",
        "\n",
        "    for it in range(iterations):\n",
        "        cost = 0.0\n",
        "        indices = np.random.permutation(m)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "        for i in range(0, m, batch_size):\n",
        "            X_i = X[i: i + batch_size]\n",
        "            y_i = y[i: i + batch_size]\n",
        "\n",
        "            X_i = np.c_[np.ones(len(X_i)), X_i]\n",
        "            prediction = np.dot(X_i, y_pred)\n",
        "\n",
        "            y_pred = y_pred - (1 / m) * learning_rate * (X_i.T.dot((prediction - y_i)))\n",
        "            cost += cal_cost(y_pred, X_i, y_i)\n",
        "\n",
        "        cost_history[it]  = cost\n",
        "\n",
        "    return y_pred, cost_history"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "SpbsVwA28znL",
        "outputId": "e28c7d64-a538-4203-b86c-a8a89df15b7f"
      },
      "source": [
        "lr = 0.1\n",
        "n_iter = 200\n",
        "y_pred = np.random.randn(2,1)\n",
        "y_pred, cost_history = minibatch_gradient_descent(X, y, y_pred, lr, n_iter)\n",
        "\n",
        "print('y_pred[0]: {:0.3f}\\ny_pred[1]: {:0.3f}'.format(y_pred[0][0], y_pred[1][0]))\n",
        "print('Final error: {:0.3f}'.format(cost_history[-1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-dea0b5f30066>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Q_ivOYHT817C",
        "outputId": "c5f7cd06-094c-4c23-8302-da97706049b9"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "\n",
        "ax.set_ylabel('Error')\n",
        "ax.set_xlabel('Number of iterations')\n",
        "y_pred = np.random.randn(2,1)\n",
        "\n",
        "ax.plot(range(n_iter), cost_history, 'b.')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3ca211439327>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things to try out:\n",
        "\n",
        "1. Change batch size in mini-batch gradient descent.\n",
        "2. Test all the three out on real datasets.\n",
        "3. Compare the effects of changing learning rate by the same amount in Batch GD, SGD and Mini-batch GD."
      ],
      "metadata": {
        "id": "0neTARjKUoP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Additional Critical Thinking Question**\n",
        "\n"
      ],
      "metadata": {
        "id": "u8BdtVjRdKOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Which of the following is TRUE, given the optimal learning rate?**\n",
        "\n",
        " (i) Batch gradient descent is always guaranteed to converge to the global optimum of a loss function.\n",
        "\n",
        " (ii) Stochastic gradient descent is always guaranteed to converge to the global optimum of a loss function.\n",
        "\n",
        " (iii) For convex loss functions (i.e. with a bowl shape), batch gradient descent is guaranteed to eventually converge to the global optimum while stochastic gradient descent is not.\n",
        "\n",
        " (iv) For convex loss functions (i.e. with a bowl shape), stochastic gradient descent is guaranteed to eventually converge to the global optimum while batch gradient descent is not.\n",
        "\n",
        " (v) For convex loss functions (i.e. with a bowl shape), both stochastic gradient descent and batch gradient descent will eventually converge to the global optimum.\n",
        "\n",
        " (vi) For convex loss functions (i.e. with a bowl shape), neither stochastic gradient descent nor batch gradient descent are guaranteed to converge to the global optimum."
      ],
      "metadata": {
        "id": "5XZRrUrffL2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct statement is:\n",
        "\n",
        "(v) For convex loss functions (i.e., with a bowl shape), both stochastic gradient descent and batch gradient descent will eventually converge to the global optimum.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Explanation:\n",
        "\n",
        "1. Batch Gradient Descent:\n",
        "\n",
        "Convex Loss Function:\n",
        "\n",
        "For a convex loss function, the surface has a single global minimum.\n",
        "\n",
        "With an optimal learning rate, batch gradient descent is guaranteed to converge to the global optimum because it computes the exact gradient of the loss function with respect to all data points in every iteration.\n",
        "\n",
        "\n",
        "\n",
        "2. Stochastic Gradient Descent:\n",
        "\n",
        "Convex Loss Function:\n",
        "\n",
        "Stochastic gradient descent (SGD) introduces randomness by updating weights using a single sample (or a small mini-batch) at each step.\n",
        "\n",
        "With a properly decaying learning rate, SGD is also guaranteed to converge to the global optimum for convex loss functions. However, due to its stochastic nature, the convergence might involve oscillations near the global optimum.\n",
        "\n",
        "\n",
        "\n",
        "3. Non-Convex Loss Functions:\n",
        "\n",
        "Neither batch gradient descent nor stochastic gradient descent is guaranteed to find the global optimum in non-convex problems (e.g., deep neural networks), as there might be multiple local minima or saddle points.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Why Other Options Are Incorrect:\n",
        "\n",
        "1. (i) Batch gradient descent is always guaranteed to converge to the global optimum of a loss function.\n",
        "\n",
        "False. This is true only for convex loss functions. For non-convex functions, it may converge to a local minimum.\n",
        "\n",
        "\n",
        "\n",
        "2. (ii) Stochastic gradient descent is always guaranteed to converge to the global optimum of a loss function.\n",
        "\n",
        "False. SGD is not guaranteed to converge to the global optimum for non-convex functions and might oscillate near minima.\n",
        "\n",
        "\n",
        "\n",
        "3. (iii) For convex loss functions, batch gradient descent is guaranteed to eventually converge to the global optimum while stochastic gradient descent is not.\n",
        "\n",
        "False. Both are guaranteed to converge for convex loss functions.\n",
        "\n",
        "\n",
        "\n",
        "4. (iv) For convex loss functions, stochastic gradient descent is guaranteed to eventually converge to the global optimum while batch gradient descent is not.\n",
        "\n",
        "False. Both batch gradient descent and stochastic gradient descent converge for convex loss functions.\n",
        "\n",
        "\n",
        "\n",
        "5. (vi) For convex loss functions, neither stochastic gradient descent nor batch gradient descent are guaranteed to converge.\n",
        "\n",
        "False. Both converge for convex loss functions."
      ],
      "metadata": {
        "id": "uoD2sLRjf2eP"
      }
    }
  ]
}